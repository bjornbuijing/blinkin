{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We will explore this dataset: https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State#\n",
    "\n",
    "> All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data_dir = \"../../data/raw\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "datapath = tf.keras.utils.get_file(\n",
    "        \"eeg\", origin=url, untar=False, cache_dir=data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the arff file with scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "data = arff.loadarff(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a tuple of a description and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 15k observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations are tuples of floats and a byte as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for x in data[0]:\n",
    "    labels.append(int(x[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 45% of the data has closed eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises 1\n",
    "\n",
    "- create a get_eeg function that downloads the data to a given path\n",
    "- build a Dataset that yields a $X, y$ tuple of tensors. $X$ should be sequential in time. Remember: a dataset should implement `__get_item__` and `__len__`.\n",
    "- note that you could model this as both a classification task, but also as a sequence-to-sequence task! For this excercise, make it a classification task with consecutive 0s or 1s only.\n",
    "- Note that, for a training task, a seq2seq model will probably be more realistic. However, the classification is a nice excercise because it is harder to set up.\n",
    "- figure out what the length distribution is of your dataset: how many timestamps do you have for every consecutive sequence of 0s and 1s? On average, median, min, max?\n",
    "- create a dataloader that yields timeseries with (batch, sequence_lenght). You can implement: windowed, padded and batched.\n",
    "    1. yielding a windowed item should be the easy level\n",
    "    2. yielding windowed and padded is medium level \n",
    "    3. yielding windowed, padded and batched is expert level, because the windowing will cause the timeseries to have different sizes. You will need to buffer before you can yield a batch.\n",
    "\n",
    "1. Upload this to github. \n",
    "2. Put your dev notebooks in a seperate folder\n",
    "3. Put all your functions in the src folder\n",
    "4. Use a formater & linter\n",
    "5. Add a single notebook, that sources the src folder. Indicate which level you got (1, 2 or 3)\n",
    "6. and that shows your dataloader works:\n",
    "    - it should not give errors because it runs out of data! Either let is stop by itself, or run forever.\n",
    "    - batchsize should be consistent (in case 1 and 2, batchsize is 1)\n",
    "    - sequence length is allowed to vary\n",
    "\n",
    "The first excercise is ex1, this one is ex2. You will get $max(ex1, average(ex1, ex2))$ as a final remark.\n",
    "Level 3 can get you an 11, because it exceeds expectation.\n",
    "\n",
    "# Excercise 2\n",
    "- build a Dataset that yields sequences of X, y. This time, y is a sequence and can contain both 0s and 1s\n",
    "- create a Dataloader with this\n",
    "- Test appropriate architectures (RNN, Attention)\n",
    "- for the loss, note that you will need a BCELoss instead of a CrossEntroyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCELoss example\n",
    "In this example, which input would you prefer for the given target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input1 = torch.tensor([0.1, 0.1, 0.7, 0.9])\n",
    "input2 = torch.tensor([0.1, 0.3, 0.6, 0.7])\n",
    "target = torch.tensor([0., 0., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which loss should you pick? CrossEntropyLoss won't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "try:\n",
    "    loss(input1, target)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need BCELoss for this.\n",
    "Binary cross entropy loss works like this:\n",
    "$$X = {x_i, \\dots, x_n}$$\n",
    "\n",
    "$$l_i =-(y_i \\cdot log(x_i) + (1-y_i) \\cdot log(1-x_i))$$\n",
    "$$BCELoss = mean(l)$$\n",
    "\n",
    "Note that the labels are assumed to be either 0 or 1 (hence, the binary part).\n",
    "If a label is 0, only the second part is relevent. If the label is 1, only the first part is relevant. the default reduction is \"mean\":\n",
    "\n",
    "$$\n",
    "BCEloss = \n",
    "\\begin{cases}\n",
    "mean(-log(1 - x_i)) & \\text{if\\,} y = 0\\\\\n",
    "mean(-log(x_i)) & \\text{if\\,} y = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see this works nice for a sequence of 0s and 1s.\n",
    "You can see that input1 is preferred, because it is more certain of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss(input1, target), loss(input2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a more generic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid() # make sure outputs are between 0 and 1\n",
    "X = torch.randn(100) # generate 100 random inputs\n",
    "yhat = m(X) # our dummy model\n",
    "\n",
    "p = torch.ones_like(yhat) / 2\n",
    "y = torch.bernoulli(p) # we create a random label sequence of 0s and 1s\n",
    "loss(yhat, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69855e6a671ebbae22bded53bf54a2e55965db9df64dc0686841d36ea41951c3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
